# Краткий обзор алгоритмов классификации, их оптимизации и анализа
___
Основная задача, которая будет здесь рассматриваться - это классифицировать объект (т.е. определить к какому классу он принадлежит) по имеющимся данным (выборке), и сделать это максимально верно.

Для начала рассмотрим алгоритмы классификации, их отличительные особенности, графические иллюстрации и то, что я допишу, надеюсь, ...
Все рассматриваемые алгоритмы метрические, т.е. опираются на некоторую функцию расстояния (в нашем случае Евклидово). Введем следующие обозначения:

 `xl` - выборка объектов 
 
 `u` - классифицируемый объект
 
 `p` - функция расстояния
 
 `Y` - класс
 
## kNN
___
Смысл алгоритма очень прост. Если в двух словах, то мы говорим, что объект `u` относится к классу `Yi`, если среди k ближайших объектов со схожими свойствами больше объектов, принадлежащих к классу `Yi`. Степень близости каждого из `k` соседей не учитывается, важен сам факт соседства. Визуально это отображено на картинке ниже:

![kNN](https://github.com/naemnamenmea/SMCS/blob/master/images/kNN.png)

[Оптимальный k для kNN](https://github.com/naemnamenmea/SMCS/blob/master/images/kNN_kOpt.png).
Код лежит [тут](https://github.com/naemnamenmea/SMCS/blob/master/sourses/kNN.R).
Подробнее про **_Метод ближайших соседей_** можно найти [здесь](www.machinelearning.ru/wiki/index.php?title=Метод_ближайшего_соседа).

## kwNN
___
Здесь уже вводится т.н. весовая фунция `w(i)`, которая в алгоритме kNN была просто константой множителем - 1. `w(i)` - убывающая функция. Тут мы опять нахожим `k` ближайших соседей, но теперь чем дальше сосед, тем меньший вклад он привносит в определении класса объекта `u`.

![kwNN](https://github.com/naemnamenmea/SMCS/blob/master/images/kwNN.png)

[Оптимальный k для kwNN](https://github.com/naemnamenmea/SMCS/blob/master/images/kwNN_kOpt.png).
Код лежит [тут](https://github.com/naemnamenmea/SMCS/blob/master/sourses/kwNN.R).
Подробнее про **_Метод взвешенных ближайших соседей_** можно найти [здесь](http://www.machinelearning.ru/wiki/index.php?title=Метод_k_взвешенных_ближайших_соседей_(пример)).

## Парзеновское окно
___
Представим что наш объект `u` является центром сферы, и мы расширили радиус сферы до значения `h` - ширины окна. Тогда чем больше объектов `Yi` класса окажется внутри этой сферы, тем вероятнее что объект `u` принадлежит классу `Yi`. По сути этот метод является двойственным к kNN (kwNN).

![Парзеновское окно](https://github.com/naemnamenmea/SMCS/blob/master/images/parsenWindowFix.png)

[Оптимальное h для Парзеновского окна](https://github.com/naemnamenmea/SMCS/blob/master/images/parsenWindowFix_hOpt.png).
Код лежит [тут](https://github.com/naemnamenmea/SMCS/blob/master/sourses/parsenWindowFix.R).
Я [рисовал](https://github.com/naemnamenmea/SMCS/blob/master/images/pasrenwindowexample.png).
Подробнее про **_метод Парзеновского окна_** можно найти [здесь](www.machinelearning.ru/wiki/index.php?title=Метод_парзеновского_окна).

## Парзеновское окно с переменной шириной
___
Здесь выбор параметра `h` зависит от `k`. Так что по большому счету это тот же kNN. Только тут вместо `w(i)`, используется `Ker(u)` - функция ядра, интеграл по которой = 1, и она также убывает на интервале `[0,oo]`.

![Парзеновское окно с переменной шириной](https://github.com/naemnamenmea/SMCS/blob/master/images/parsenWindowFloat.png)

[Оптимальное k для Парзеновского окна с переменной шириной](https://github.com/naemnamenmea/SMCS/blob/master/images/parsenWindowFloat_kOpt.png).
Код лежит [тут](https://github.com/naemnamenmea/SMCS/blob/master/sourses/parsenWindowFloat.R).
Подробнее про **_метод Парзеновского окна с переменной шириной_** можно найти [здесь](machinelearning.ru/wiki/index.php?title=Метод_Парзеновского_окна_(пример)).

## Метод потенциальных функций
___
Тут тоже все несложно. Мы проводим аналогию с физическими частицами (они имеют заряд и радиус действия этого заряда) и полагаем что каждый объект выборки `xl` имеет некий потенциал (заряд) и расстояние его действия. Каждый `i`-ый объект вносит в долю своего класса значение, которое равно `потенциал * ( степень действия/расстояние до объекта u )`.

Код лежит [тут](https://github.com/naemnamenmea/SMCS/blob/master/sourses/potential.R).
Подробнее про **_метод потенциальных функций_** можно найти [здесь](www.machinelearning.ru/wiki/index.php?title=Метод_потенциальных_функций).

Полезно ознакомиться с таблицей ниже...

| Алгоритм                     | Оптимальный параметр |
| ---------------------------- |:--------------------:|
| kNN                          | k = 6                |
| kwNN                         | k = 4                |
| Парзеновское окно (h=const)  | h = 1                |
| Парзеновское окно h(k)       | h = 32               |
| Потенциальные функции        | ??                   |


### Сравнение алгоритмов:
![сравнение алгоритмов](https://github.com/naemnamenmea/SMCS/blob/master/images/comparison.png)

## Margin (отступы)
___
Грубо говоря `Margin =` степень близости объекта `u` до своего класса `-` степень близости до ближайшего НЕ своего класса.
Посчитаем отступы всех элементов выборки `xl`, расположим объекты в порядке возрастания отступов и построим их график:

![margin](https://github.com/naemnamenmea/SMCS/blob/master/images/margin.png)
Код лежит [тут](https://github.com/naemnamenmea/SMCS/blob/master/sourses/Margin.R).
Подробнее про **_Отступы_** и их применение можно найти [здесь](ru.learnmachinelearning.wikia.com/wiki/Отступ_(для_классификатора)).

## STOLP
___
Этот метод (оптимизационный) позволяет сократить выборку `xl` выбросив из нее шумовые, неинформативные объекты, используя какой-то определенный алгоритм, например, `kNN` и функцию отступов `Margin`. Думаю не нужно объяснять почему он так хорош...

![STOLP](https://github.com/naemnamenmea/SMCS/blob/master/images/STOLP.png)
Код лежит [тут](https://github.com/naemnamenmea/SMCS/blob/master/sourses/STOLP.R).
Подробнее про алгоритм **_STOLP_** можно найти [здесь](http://www.machinelearning.ru/wiki/index.php?title=Машинное_обучение_(курс_лекций%2C_К.В.Воронцов)).

## Примечания
___
`3` алгоритма ведут себя одинаково при `k<40`,
только алгоритм Парзеновского окна с h=const начинает экспоненциально ошибаться с ростом `h`

На картинке "`parsenWindowFix_hOpt`" видно, что `LOO(h)` очень быстро стремится к `1`

При построении алгоритма `STOLP` возник 1 нюанс: когда мы проходимся по объектам,
на которых алгоритм a допускает ошибку, и считаем для них отступы, может оказаться что
они все `> 0` и не произойдет добавления нового эл. `x_i` к `Omega`. Для решения этой проблемы
была введена дополнительная переменная `delta(=0 по default)`, которую мы используем при
проверке отступов и увеличиваем на небольшую константу.

Метод "парзеновского окна" с `K(z)=1/2` , при `|z|<=1` тоже самое что и kNN с `w(i,u)=const`

Метод "парзеновского окна" с `K(z)!=const` тоже самое что и kwNN, где `w(i,u)!=const`

Эти алгоритмы принципиально различаются только в одном: в kNN мы неявно задаем `h` (ширину окна) через `k`,
а в "парзеновском окне", наоборот, - мы неявно задаем `k` через `h`.

в kNN полагается `p(u,x_i)=1`

ф-ция расстояния `p(u,x_i)` это частный случай весовой ф-ции `w(i,u)`
