Кривая ошибок или ROC-кривая – графичекая характеристика качества бинарного вероятностного классификатора, зависимость доли верных положительных классификаций от доли ложных положительных классификаций при варьировании порога решающего правила. Численная характеристика ROC - AUC (Area under curve), чем выше значение - тем лучше, 0.5 - плохая классификация, не отличающаяся от равновероятностной, больше 0.75 - считается хорошая классификация.

ROC-кривая всегда начинается в (0, 0) и заканчивается в (1, 1).

Для численного представления качества оценки используют матрицу ошибок (confusion matrix):

|---|Predicted = 0|Predicted = 1|
|---|---|---|
|Actual = 0|True Negatives (TN)|False Positives (FP)|
|Actual = 1|False Negatives (FN)|True Positives (TP)|

В случае бинарной классификации метка класса y принимает значение '1' (positive) или '0' (negative). Вводятся 4 величины, соответствующие элементам матрицы ошибок:

* True positive (TP).
* True negative (TN).
* False positive (FP).
* False negative (FN).

P означает что классификатор определяет класс объекта как положительный (N - отрицательный). T - значит что класс предсказан правильно (соответственно F - неправильно).

Отсюда можно получить два значения, которые помогут нам определить какие ошибки делает модель:

_Sensitivity_ = TP/(TP+FN) - (~recall), или **True positive rate (TPR)**. Процент верно предсказанных классов у объектов, относящихся к классу '1'.

_Specificity_ = TN/(TN+FP) - **True negative rate (TNR)**. Процент верно предсказанных классов у объектов, относящихся к классу '0'.

Модель с более высоким пороговым значением будет иметь более высокую чувствительность и низкую специфичность. Модель с низким пороговым значением наоборот.

False Positive Rate (FPR) = FP/(FP+TN) - это доля неправильно предсказанных классов среди объектов класса '0'.
False Negative Rate (FNR) = FN/(FN+TP)
Precision = TP/(TP+FP)